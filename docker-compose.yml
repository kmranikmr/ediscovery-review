version: '3.8'

services:
  # Ollama service with GPU support
  ollama:
    image: ollama/ollama:latest
    container_name: staging-ollama-gpu
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    networks:
      - ai-network
    restart: unless-stopped
    command: >
      sh -c "ollama serve & 
             sleep 15 && 
             ollama pull mistral && 
             wait"

  # API Server (with Ollama integration)
  api:
    build: .
    container_name: staging-api-test
    ports:
      - "8001:8001"
    environment:
      - HOST=0.0.0.0
      - PORT=8001
      - PYTHONPATH=/app
      - PYTHONUNBUFFERED=1
      - TOKENIZERS_PARALLELISM=false
      - HF_HOME=/app/cache/huggingface
      # External dependencies configuration
      - SKIP_OPENSEARCH=true
      - SKIP_REDIS=true
      - SKIP_OLLAMA=false
      - OLLAMA_BASE_URL=http://ollama:11434
      - DOCKER_ENV=true
    volumes:
      - ./data:/app/data
      - ./logs:/app/logs
      - ./cache:/app/cache
      - ./models:/app/models
      - model_cache:/root/.cache/huggingface
    depends_on:
      - ollama
    networks:
      - ai-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 90s

  # Streamlit UI (Optional - can be started separately)
  streamlit:
    build: .
    container_name: staging-streamlit-test
    ports:
      - "8505:8505"
    environment:
      - PYTHONPATH=/app
      - API_BASE_URL=http://api:8001
    command: ["python", "-m", "streamlit", "run", "streamlit/app.py", "--server.port=8505", "--server.address=0.0.0.0"]
    volumes:
      - ./streamlit:/app/streamlit
      - ./data:/app/data
    depends_on:
      - api
    networks:
      - ai-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/api/v1/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Streamlit UI
  streamlit:
    build:
      context: .
      dockerfile: docker/Dockerfile.streamlit
    container_name: ediscovery-streamlit
    ports:
      - "8501:8501"
    environment:
      - API_BASE_URL=http://api:8001/api/v1
    depends_on:
      - api
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8501/_stcore/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # OpenSearch
  opensearch:
    image: opensearchproject/opensearch:2.11.0
    container_name: ediscovery-opensearch
    environment:
      - cluster.name=ediscovery-cluster
      - node.name=ediscovery-node
      - discovery.type=single-node
      - bootstrap.memory_lock=true
      - "OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m"
      - "DISABLE_INSTALL_DEMO_CONFIG=true"
      - "DISABLE_SECURITY_PLUGIN=true"
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    volumes:
      - opensearch-data:/usr/share/opensearch/data
    ports:
      - "9200:9200"
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9200/_cluster/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Redis (for Celery tasks)
  redis:
    image: redis:7-alpine
    container_name: ediscovery-redis
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Ollama (external service - include if running locally)
  # ollama:
  #   image: ollama/ollama:latest
  #   container_name: ediscovery-ollama
  #   ports:
  #     - "11434:11434"
  #   volumes:
  #     - ollama-data:/root/.ollama
  #   restart: unless-stopped

  # Celery Worker (for future async tasks)
  # celery-worker:
  #   build: .
  #   container_name: ediscovery-celery-worker
  #   command: celery -A app.core.celery worker --loglevel=info
  #   environment:
  #     - CELERY_BROKER_URL=redis://redis:6379/0
  #     - CELERY_RESULT_BACKEND=redis://redis:6379/0
  #   depends_on:
  #     - redis
  #   restart: unless-stopped

volumes:
  opensearch-data:
    driver: local
  redis-data:
    driver: local
  ollama_data:
    driver: local
  model_cache:
    driver: local

networks:
  ai-network:
    driver: bridge

networks:
  default:
    name: ediscovery-network
